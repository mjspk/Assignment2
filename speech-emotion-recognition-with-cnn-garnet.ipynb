{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# Including Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-01T06:49:46.47379Z",
     "iopub.status.busy": "2022-07-01T06:49:46.473068Z",
     "iopub.status.idle": "2022-07-01T06:50:00.292595Z",
     "shell.execute_reply": "2022-07-01T06:50:00.291134Z",
     "shell.execute_reply.started": "2022-07-01T06:49:46.473685Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "# import keras.layers as L\n",
    "# import tensorflow as tf\n",
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau \n",
    "# from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "# import re\n",
    "# import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# Including the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:08.747833Z",
     "iopub.status.busy": "2022-07-01T06:51:08.746781Z",
     "iopub.status.idle": "2022-07-01T06:51:08.754674Z",
     "shell.execute_reply": "2022-07-01T06:51:08.75327Z",
     "shell.execute_reply.started": "2022-07-01T06:51:08.747759Z"
    }
   },
   "outputs": [],
   "source": [
    "main_Path='Emotions/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# Data Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:10.050969Z",
     "iopub.status.busy": "2022-07-01T06:51:10.050559Z",
     "iopub.status.idle": "2022-07-01T06:51:10.488245Z",
     "shell.execute_reply": "2022-07-01T06:51:10.487013Z",
     "shell.execute_reply.started": "2022-07-01T06:51:10.050937Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "emotions=os.listdir(main_Path)\n",
    "main=[]\n",
    "for emotion in emotions:\n",
    "    path=main_Path+emotion+'/'\n",
    "    for file in os.listdir(path):\n",
    "        main.append([emotion,file])\n",
    "main_df=pd.DataFrame(main,columns=['Emotion','File'])\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:11.544958Z",
     "iopub.status.busy": "2022-07-01T06:51:11.544557Z",
     "iopub.status.idle": "2022-07-01T06:51:11.5626Z",
     "shell.execute_reply": "2022-07-01T06:51:11.561231Z",
     "shell.execute_reply.started": "2022-07-01T06:51:11.54493Z"
    }
   },
   "outputs": [],
   "source": [
    "main_df['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:17.055659Z",
     "iopub.status.busy": "2022-07-01T06:51:17.054308Z",
     "iopub.status.idle": "2022-07-01T06:51:17.081003Z",
     "shell.execute_reply": "2022-07-01T06:51:17.079654Z",
     "shell.execute_reply.started": "2022-07-01T06:51:17.055614Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "main_df.to_csv('main_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:17.691118Z",
     "iopub.status.busy": "2022-07-01T06:51:17.690313Z",
     "iopub.status.idle": "2022-07-01T06:51:17.709576Z",
     "shell.execute_reply": "2022-07-01T06:51:17.708452Z",
     "shell.execute_reply.started": "2022-07-01T06:51:17.691077Z"
    }
   },
   "outputs": [],
   "source": [
    "main_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:18.282811Z",
     "iopub.status.busy": "2022-07-01T06:51:18.281932Z",
     "iopub.status.idle": "2022-07-01T06:51:18.296734Z",
     "shell.execute_reply": "2022-07-01T06:51:18.295367Z",
     "shell.execute_reply.started": "2022-07-01T06:51:18.282739Z"
    }
   },
   "outputs": [],
   "source": [
    "main_df.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:19.038254Z",
     "iopub.status.busy": "2022-07-01T06:51:19.037371Z",
     "iopub.status.idle": "2022-07-01T06:51:19.412397Z",
     "shell.execute_reply": "2022-07-01T06:51:19.410619Z",
     "shell.execute_reply.started": "2022-07-01T06:51:19.038211Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Emotions Counts')\n",
    "emotions=sns.countplot(x='Emotion',data=main_df,palette='Set2')\n",
    "emotions.set_xticklabels(emotions.get_xticklabels(),rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:19.76866Z",
     "iopub.status.busy": "2022-07-01T06:51:19.76762Z",
     "iopub.status.idle": "2022-07-01T06:51:19.776989Z",
     "shell.execute_reply": "2022-07-01T06:51:19.77583Z",
     "shell.execute_reply.started": "2022-07-01T06:51:19.768614Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion_names=main_df['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:20.603197Z",
     "iopub.status.busy": "2022-07-01T06:51:20.602686Z",
     "iopub.status.idle": "2022-07-01T06:51:20.609851Z",
     "shell.execute_reply": "2022-07-01T06:51:20.607527Z",
     "shell.execute_reply.started": "2022-07-01T06:51:20.603168Z"
    }
   },
   "outputs": [],
   "source": [
    "colors={'Disgusted':'#804E2D','Happy':'#F19C0E','Sad':'#478FB8','Neutral':'#4CB847','Fearful':'#7D55AA','Angry':'#C00808','Suprised':'#EE00FF'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:21.369993Z",
     "iopub.status.busy": "2022-07-01T06:51:21.369654Z",
     "iopub.status.idle": "2022-07-01T06:51:21.378797Z",
     "shell.execute_reply": "2022-07-01T06:51:21.377474Z",
     "shell.execute_reply.started": "2022-07-01T06:51:21.369966Z"
    }
   },
   "outputs": [],
   "source": [
    "def wave_plot(data,sr,emotion,color):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.title(f'{emotion} emotion for waveplot',size=17)\n",
    "    librosa.display.waveshow(y=data,sr=sr,color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:22.307546Z",
     "iopub.status.busy": "2022-07-01T06:51:22.307162Z",
     "iopub.status.idle": "2022-07-01T06:51:22.315709Z",
     "shell.execute_reply": "2022-07-01T06:51:22.313711Z",
     "shell.execute_reply.started": "2022-07-01T06:51:22.307518Z"
    }
   },
   "outputs": [],
   "source": [
    "def spectogram(data,sr,emotion):\n",
    "    audio=librosa.stft(data)\n",
    "    audio_db=librosa.amplitude_to_db(abs(audio))\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.title(f'{emotion} emotion for spectogram',size=17)\n",
    "    librosa.display.specshow(audio_db,sr=sr,x_axis='time',y_axis='hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:23.044953Z",
     "iopub.status.busy": "2022-07-01T06:51:23.043677Z",
     "iopub.status.idle": "2022-07-01T06:51:29.439099Z",
     "shell.execute_reply": "2022-07-01T06:51:29.437818Z",
     "shell.execute_reply.started": "2022-07-01T06:51:23.044907Z"
    }
   },
   "outputs": [],
   "source": [
    "audio_path=[]\n",
    "for emotion in emotion_names:\n",
    "    path=main_Path+emotion+ \"/\" + np.array(main_df['File'][main_df['Emotion']==emotion])[1]\n",
    "    data,sr=librosa.load(path)\n",
    "    wave_plot(data,sr,emotion,colors[emotion])\n",
    "    spectogram(data,sr,emotion)\n",
    "    audio_path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:29.442522Z",
     "iopub.status.busy": "2022-07-01T06:51:29.441748Z",
     "iopub.status.idle": "2022-07-01T06:51:29.460924Z",
     "shell.execute_reply": "2022-07-01T06:51:29.459467Z",
     "shell.execute_reply.started": "2022-07-01T06:51:29.442477Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Disgust Audio Sample\\n')\n",
    "IPython.display.Audio(audio_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:29.463587Z",
     "iopub.status.busy": "2022-07-01T06:51:29.46267Z",
     "iopub.status.idle": "2022-07-01T06:51:29.479849Z",
     "shell.execute_reply": "2022-07-01T06:51:29.478014Z",
     "shell.execute_reply.started": "2022-07-01T06:51:29.463546Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Happy Audio Sample\\n')\n",
    "IPython.display.Audio(audio_path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:29.484182Z",
     "iopub.status.busy": "2022-07-01T06:51:29.483177Z",
     "iopub.status.idle": "2022-07-01T06:51:29.501258Z",
     "shell.execute_reply": "2022-07-01T06:51:29.499741Z",
     "shell.execute_reply.started": "2022-07-01T06:51:29.484123Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Sad Audio Sample\\n')\n",
    "IPython.display.Audio(audio_path[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:29.5039Z",
     "iopub.status.busy": "2022-07-01T06:51:29.503101Z",
     "iopub.status.idle": "2022-07-01T06:51:29.517199Z",
     "shell.execute_reply": "2022-07-01T06:51:29.515817Z",
     "shell.execute_reply.started": "2022-07-01T06:51:29.503855Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Neutral Audio Sample\\n')\n",
    "IPython.display.Audio(audio_path[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:29.520224Z",
     "iopub.status.busy": "2022-07-01T06:51:29.519137Z",
     "iopub.status.idle": "2022-07-01T06:51:29.533412Z",
     "shell.execute_reply": "2022-07-01T06:51:29.532045Z",
     "shell.execute_reply.started": "2022-07-01T06:51:29.520185Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Fear Audio Sample\\n')\n",
    "IPython.display.Audio(audio_path[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:29.536496Z",
     "iopub.status.busy": "2022-07-01T06:51:29.53562Z",
     "iopub.status.idle": "2022-07-01T06:51:29.549455Z",
     "shell.execute_reply": "2022-07-01T06:51:29.547603Z",
     "shell.execute_reply.started": "2022-07-01T06:51:29.536456Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Angry Audio Sample\\n')\n",
    "IPython.display.Audio(audio_path[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:29.552259Z",
     "iopub.status.busy": "2022-07-01T06:51:29.551846Z",
     "iopub.status.idle": "2022-07-01T06:51:29.572712Z",
     "shell.execute_reply": "2022-07-01T06:51:29.571644Z",
     "shell.execute_reply.started": "2022-07-01T06:51:29.552222Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Surprise Audio Sample\\n')\n",
    "IPython.display.Audio(audio_path[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## Audio Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:36.976101Z",
     "iopub.status.busy": "2022-07-01T06:51:36.975653Z",
     "iopub.status.idle": "2022-07-01T06:51:36.98801Z",
     "shell.execute_reply": "2022-07-01T06:51:36.986641Z",
     "shell.execute_reply.started": "2022-07-01T06:51:36.976071Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_noise(data,random=False,rate=0.035,threshold=0.075):\n",
    "    if random:\n",
    "        rate=np.random.random()*threshold\n",
    "    noise=rate*np.random.uniform()*np.amax(data)\n",
    "    augmented_data=data+noise*np.random.normal(size=data.shape[0])\n",
    "    return augmented_data\n",
    "\n",
    "def shifting(data,rate=1000):\n",
    "    augmented_data=int(np.random.uniform(low=-5,high=5)*rate)\n",
    "    augmented_data=np.roll(data,augmented_data)\n",
    "    return augmented_data\n",
    "\n",
    "def pitching(data,sr,pitch_factor=0.7,random=False):\n",
    "    if random:\n",
    "        pitch_factor=np.random.random() * pitch_factor\n",
    "    return librosa.effects.pitch_shift(data,sr=sr,n_steps=pitch_factor)\n",
    "\n",
    "def streching(data,rate=0.8):\n",
    "    return librosa.effects.time_stretch(data,rate=rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:37.657256Z",
     "iopub.status.busy": "2022-07-01T06:51:37.656925Z",
     "iopub.status.idle": "2022-07-01T06:51:37.883359Z",
     "shell.execute_reply": "2022-07-01T06:51:37.881967Z",
     "shell.execute_reply.started": "2022-07-01T06:51:37.657228Z"
    }
   },
   "outputs": [],
   "source": [
    "data,sr=librosa.load(audio_path[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "### Original Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:38.675218Z",
     "iopub.status.busy": "2022-07-01T06:51:38.674083Z",
     "iopub.status.idle": "2022-07-01T06:51:39.085816Z",
     "shell.execute_reply": "2022-07-01T06:51:39.084426Z",
     "shell.execute_reply.started": "2022-07-01T06:51:38.675161Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "librosa.display.waveshow(data,sr=sr,color='#EE00FF')\n",
    "IPython.display.Audio(audio_path[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "### Noised Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:42.782801Z",
     "iopub.status.busy": "2022-07-01T06:51:42.782373Z",
     "iopub.status.idle": "2022-07-01T06:51:43.19918Z",
     "shell.execute_reply": "2022-07-01T06:51:43.19796Z",
     "shell.execute_reply.started": "2022-07-01T06:51:42.782747Z"
    }
   },
   "outputs": [],
   "source": [
    "noised_audio=add_noise(data)\n",
    "plt.figure(figsize=(12,5))\n",
    "librosa.display.waveshow(noised_audio,sr=sr,color='#EE00FF')\n",
    "IPython.display.Audio(noised_audio,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "### Streched Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:43.532003Z",
     "iopub.status.busy": "2022-07-01T06:51:43.530696Z",
     "iopub.status.idle": "2022-07-01T06:51:44.43938Z",
     "shell.execute_reply": "2022-07-01T06:51:44.437922Z",
     "shell.execute_reply.started": "2022-07-01T06:51:43.531957Z"
    }
   },
   "outputs": [],
   "source": [
    "stretched_audio=streching(data)\n",
    "plt.figure(figsize=(12,5))\n",
    "librosa.display.waveshow(stretched_audio,sr=sr,color='#EE00FF')\n",
    "IPython.display.Audio(stretched_audio,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a>\n",
    "### Shifted Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:44.618374Z",
     "iopub.status.busy": "2022-07-01T06:51:44.618045Z",
     "iopub.status.idle": "2022-07-01T06:51:45.021885Z",
     "shell.execute_reply": "2022-07-01T06:51:45.020299Z",
     "shell.execute_reply.started": "2022-07-01T06:51:44.618346Z"
    }
   },
   "outputs": [],
   "source": [
    "shifted_audio=shifting(data)\n",
    "plt.figure(figsize=(12,5))\n",
    "librosa.display.waveshow(shifted_audio,sr=sr,color='#EE00FF')\n",
    "IPython.display.Audio(shifted_audio,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a>\n",
    "### Pitched Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:47.524677Z",
     "iopub.status.busy": "2022-07-01T06:51:47.523566Z",
     "iopub.status.idle": "2022-07-01T06:51:48.088372Z",
     "shell.execute_reply": "2022-07-01T06:51:48.086832Z",
     "shell.execute_reply.started": "2022-07-01T06:51:47.524618Z"
    }
   },
   "outputs": [],
   "source": [
    "pitched_audio=pitching(data,sr)\n",
    "plt.figure(figsize=(12,5))\n",
    "librosa.display.waveshow(pitched_audio,sr=sr,color='#EE00FF')\n",
    "IPython.display.Audio(pitched_audio,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"13\"></a>\n",
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:48.725835Z",
     "iopub.status.busy": "2022-07-01T06:51:48.725479Z",
     "iopub.status.idle": "2022-07-01T06:51:48.751371Z",
     "shell.execute_reply": "2022-07-01T06:51:48.749839Z",
     "shell.execute_reply.started": "2022-07-01T06:51:48.725805Z"
    }
   },
   "outputs": [],
   "source": [
    "def zcr(data,frame_length,hop_length):\n",
    "    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "def rmse(data,frame_length=2048,hop_length=512):\n",
    "    rmse=librosa.feature.rms(y=data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(rmse)\n",
    "def mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n",
    "    mfcc=librosa.feature.mfcc(y=data,sr=sr)\n",
    "    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n",
    "\n",
    "def extract_features(data,sr,frame_length=2048,hop_length=512):\n",
    "    result=np.array([])\n",
    "    \n",
    "    result=np.hstack((result,\n",
    "                      zcr(data,frame_length,hop_length),\n",
    "                      rmse(data,frame_length,hop_length),\n",
    "                      mfcc(data,sr,frame_length,hop_length)\n",
    "                     ))\n",
    "    return result\n",
    "\n",
    "def get_features(path,duration=2.5, offset=0.6):\n",
    "    data,sr=librosa.load(path,duration=duration,offset=offset)\n",
    "    aud=extract_features(data,sr)\n",
    "    audio=np.array(aud)\n",
    "    \n",
    "    noised_audio=add_noise(data,random=True)\n",
    "    aud2=extract_features(noised_audio,sr)\n",
    "    audio=np.vstack((audio,aud2))\n",
    "    \n",
    "    pitched_audio=pitching(data,sr,random=True)\n",
    "    aud3=extract_features(pitched_audio,sr)\n",
    "    audio=np.vstack((audio,aud3))\n",
    "    \n",
    "    pitched_audio1=pitching(data,sr,random=True)\n",
    "    pitched_noised_audio=add_noise(pitched_audio1,random=True)\n",
    "    aud4=extract_features(pitched_noised_audio,sr)\n",
    "    audio=np.vstack((audio,aud4))\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"14\"></a>\n",
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:08.281701Z",
     "iopub.status.busy": "2022-07-01T08:36:08.281392Z",
     "iopub.status.idle": "2022-07-01T08:36:09.527267Z",
     "shell.execute_reply": "2022-07-01T08:36:09.525855Z",
     "shell.execute_reply.started": "2022-07-01T08:36:08.281674Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,random_state=42,test_size=0.1,shuffle=True)\n",
    "X_train.shape, X_test.shape, X_val.shape, y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:09.53082Z",
     "iopub.status.busy": "2022-07-01T08:36:09.530003Z",
     "iopub.status.idle": "2022-07-01T08:36:11.601621Z",
     "shell.execute_reply": "2022-07-01T08:36:11.600193Z",
     "shell.execute_reply.started": "2022-07-01T08:36:09.530762Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "X_val=scaler.transform(X_val)\n",
    "X_train.shape,X_test.shape,X_val.shape,y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:11.609841Z",
     "iopub.status.busy": "2022-07-01T08:36:11.606986Z",
     "iopub.status.idle": "2022-07-01T08:36:11.626435Z",
     "shell.execute_reply": "2022-07-01T08:36:11.624048Z",
     "shell.execute_reply.started": "2022-07-01T08:36:11.609765Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train=np.expand_dims(X_train,axis=2)\n",
    "X_val=np.expand_dims(X_val,axis=2)\n",
    "X_test=np.expand_dims(X_test,axis=2)\n",
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T06:51:50.693309Z",
     "iopub.status.busy": "2022-07-01T06:51:50.6925Z",
     "iopub.status.idle": "2022-07-01T08:32:04.52093Z",
     "shell.execute_reply": "2022-07-01T08:32:04.518435Z",
     "shell.execute_reply.started": "2022-07-01T06:51:50.693264Z"
    }
   },
   "outputs": [],
   "source": [
    "X,Y=[],[]\n",
    "for path,emotion,index in zip(main_df.File,main_df.Emotion,range(main_df.File.shape[0])):\n",
    "    features=get_features(main_Path + emotion + \"/\" + path)\n",
    "    if index%500==0:\n",
    "        print(f'{index} audio has been processed')\n",
    "    for i in features:\n",
    "        X.append(i)\n",
    "        Y.append(emotion)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:32:04.58217Z",
     "iopub.status.busy": "2022-07-01T08:32:04.57656Z",
     "iopub.status.idle": "2022-07-01T08:35:32.881802Z",
     "shell.execute_reply": "2022-07-01T08:35:32.880519Z",
     "shell.execute_reply.started": "2022-07-01T08:32:04.582115Z"
    }
   },
   "outputs": [],
   "source": [
    "extract=pd.DataFrame(X)\n",
    "extract['Emotion']=Y\n",
    "extract.to_csv(\"processed_data.csv\",index=False)\n",
    "extract.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"15\"></a>\n",
    "# Including Analyzed Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:35:32.888062Z",
     "iopub.status.busy": "2022-07-01T08:35:32.885975Z",
     "iopub.status.idle": "2022-07-01T08:36:05.211223Z",
     "shell.execute_reply": "2022-07-01T08:36:05.209921Z",
     "shell.execute_reply.started": "2022-07-01T08:35:32.888016Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"processed_data.csv\")\n",
    "df.shape\n",
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:05.214109Z",
     "iopub.status.busy": "2022-07-01T08:36:05.213361Z",
     "iopub.status.idle": "2022-07-01T08:36:06.377511Z",
     "shell.execute_reply": "2022-07-01T08:36:06.375985Z",
     "shell.execute_reply.started": "2022-07-01T08:36:05.214064Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df.fillna(0)\n",
    "print(df.isna().any())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"16\"></a>\n",
    "# Processing Analyzed Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:06.424693Z",
     "iopub.status.busy": "2022-07-01T08:36:06.42386Z",
     "iopub.status.idle": "2022-07-01T08:36:06.790697Z",
     "shell.execute_reply": "2022-07-01T08:36:06.789404Z",
     "shell.execute_reply.started": "2022-07-01T08:36:06.424633Z"
    }
   },
   "outputs": [],
   "source": [
    "X=df.drop(labels='Emotion',axis=1)\n",
    "Y=df['Emotion']\n",
    "\n",
    "#Added conversion to number\n",
    "indexes = {'Disgusted':0,'Happy':1,'Sad':2,'Neutral':3,'Fearful':4,'Angry':5,'Suprised':6}\n",
    "Y = Y.map(indexes)\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:06.792976Z",
     "iopub.status.busy": "2022-07-01T08:36:06.792435Z",
     "iopub.status.idle": "2022-07-01T08:36:06.821801Z",
     "shell.execute_reply": "2022-07-01T08:36:06.819587Z",
     "shell.execute_reply.started": "2022-07-01T08:36:06.792937Z"
    }
   },
   "outputs": [],
   "source": [
    "# lb=LabelEncoder()\n",
    "# Y=np_utils.to_categorical(lb.fit_transform(Y.astype(str)))\n",
    "# print(lb.classes_)\n",
    "# Y\n",
    "\n",
    "import torch\n",
    "lb=LabelEncoder()\n",
    "Y = torch.tensor(Y)\n",
    "Y = lb.fit_transform(Y)\n",
    "Y = torch.nn.functional.one_hot(torch.tensor(Y)).float()\n",
    "print(lb.classes_)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"17\"></a>\n",
    "## Train, Test and Validation Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:06.826304Z",
     "iopub.status.busy": "2022-07-01T08:36:06.825315Z",
     "iopub.status.idle": "2022-07-01T08:36:08.275313Z",
     "shell.execute_reply": "2022-07-01T08:36:08.273808Z",
     "shell.execute_reply.started": "2022-07-01T08:36:06.826262Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,random_state=42,test_size=0.2,shuffle=True)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:08.281701Z",
     "iopub.status.busy": "2022-07-01T08:36:08.281392Z",
     "iopub.status.idle": "2022-07-01T08:36:09.527267Z",
     "shell.execute_reply": "2022-07-01T08:36:09.525855Z",
     "shell.execute_reply.started": "2022-07-01T08:36:08.281674Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,random_state=42,test_size=0.1,shuffle=True)\n",
    "X_train.shape, X_test.shape, X_val.shape, y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:09.53082Z",
     "iopub.status.busy": "2022-07-01T08:36:09.530003Z",
     "iopub.status.idle": "2022-07-01T08:36:11.601621Z",
     "shell.execute_reply": "2022-07-01T08:36:11.600193Z",
     "shell.execute_reply.started": "2022-07-01T08:36:09.530762Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "X_val=scaler.transform(X_val)\n",
    "X_train.shape,X_test.shape,X_val.shape,y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:11.609841Z",
     "iopub.status.busy": "2022-07-01T08:36:11.606986Z",
     "iopub.status.idle": "2022-07-01T08:36:11.626435Z",
     "shell.execute_reply": "2022-07-01T08:36:11.624048Z",
     "shell.execute_reply.started": "2022-07-01T08:36:11.609765Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train=np.expand_dims(X_train,axis=2)\n",
    "X_val=np.expand_dims(X_val,axis=2)\n",
    "X_test=np.expand_dims(X_test,axis=2)\n",
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"19\"></a>\n",
    "# Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T09:22:38.928719Z",
     "iopub.status.busy": "2022-07-01T09:22:38.928325Z",
     "iopub.status.idle": "2022-07-01T09:22:39.149113Z",
     "shell.execute_reply": "2022-07-01T09:22:39.147832Z",
     "shell.execute_reply.started": "2022-07-01T09:22:38.928691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioModel(\n",
       "  (conv1): Conv1d(1, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool1d(kernel_size=5, stride=2, padding=2, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool1d(kernel_size=5, stride=2, padding=2, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(512, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool1d(kernel_size=5, stride=2, padding=2, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool4): MaxPool1d(kernel_size=5, stride=2, padding=2, dilation=1, ceil_mode=False)\n",
       "  (conv5): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool5): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=9600, out_features=512, bias=True)\n",
       "  (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model=tf.keras.Sequential([\n",
    "#     L.Conv1D(512,kernel_size=5, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n",
    "#     L.BatchNormalization(),\n",
    "#     L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n",
    "#     L.Conv1D(512,kernel_size=5,strides=1,padding='same',activation='relu'),\n",
    "#     L.BatchNormalization(),\n",
    "#     L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n",
    "#     L.Conv1D(256,kernel_size=5,strides=1,padding='same',activation='relu'),\n",
    "#     L.BatchNormalization(),\n",
    "#     L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n",
    "#     L.Conv1D(256,kernel_size=3,strides=1,padding='same',activation='relu'),\n",
    "#     L.BatchNormalization(),\n",
    "#     L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n",
    "#     L.Conv1D(128,kernel_size=3,strides=1,padding='same',activation='relu'),\n",
    "#     L.BatchNormalization(),\n",
    "#     L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n",
    "#     L.Flatten(),\n",
    "#     L.Dense(512,activation='relu'),\n",
    "#     L.BatchNormalization(),\n",
    "#     L.Dense(7,activation='softmax')\n",
    "# ])\n",
    "# model.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\n",
    "\n",
    "\n",
    "# New code to define model in pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioModel, self).__init__()\n",
    "        \n",
    "        # Define the model to use\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=512, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
    "        \n",
    "        self.conv5 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "        self.pool5 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=128*75, out_features=512) # adjust the input size\n",
    "        self.bn6 = nn.BatchNorm1d(512) # adjust the input size\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.bn1(F.relu(self.conv1(x))))\n",
    "        x = self.pool2(self.bn2(F.relu(self.conv2(x))))\n",
    "        x = self.pool3(self.bn3(F.relu(self.conv3(x))))\n",
    "        x = self.pool4(self.bn4(F.relu(self.conv4(x))))\n",
    "        x = self.pool5(self.bn5(F.relu(self.conv5(x))))\n",
    "        x = x.view(-1, 128*75) # flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.softmax(x, dim=1) # add softmax activation\n",
    "        return x\n",
    "    # def __init__(self, learning_rate=2e-4):\n",
    "    #     super(AudioModel, self).__init__()\n",
    "        \n",
    "    #     # Define the model to use\n",
    "    #     self.conv1 = nn.Conv1d(in_channels=1, out_channels=512, kernel_size=5, stride=1, padding=2)\n",
    "    #     self.bn1 = nn.BatchNorm1d(512)\n",
    "    #     self.pool1 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
    "        \n",
    "    #     self.conv2 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=5, stride=1, padding=2)\n",
    "    #     self.bn2 = nn.BatchNorm1d(512)\n",
    "    #     self.pool2 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
    "        \n",
    "    #     self.conv3 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=5, stride=1, padding=2)\n",
    "    #     self.bn3 = nn.BatchNorm1d(256)\n",
    "    #     self.pool3 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
    "        \n",
    "    #     self.conv4 = nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "    #     self.bn4 = nn.BatchNorm1d(256)\n",
    "    #     self.pool4 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
    "        \n",
    "    #     self.conv5 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "    #     self.bn5 = nn.BatchNorm1d(128)\n",
    "    #     self.pool5 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    #     self.fc1 = nn.Linear(in_features=75, out_features=512)\n",
    "    #     # self.fc1 = nn.Linear(in_features=128*16, out_features=512)\n",
    "    #     self.bn6 = nn.BatchNorm1d(128)\n",
    "        \n",
    "    #     self.fc2 = nn.Linear(in_features=512, out_features=7)\n",
    "    #     self.out = nn.Conv1d(128, 1, 1)\n",
    "    #     self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    #     # Model from tutorial start with something I know works\n",
    "    #     # self.conv1 = nn.Conv1d(in_channels = 1, out_channels=60, kernel_size=3, padding=1)\n",
    "    #     # self.conv2 = nn.Conv1d(in_channels=60, out_channels=60, kernel_size=3, padding=1)\n",
    "    #     # self.conv3 = nn.Conv1d(in_channels=60, out_channels=60, kernel_size=3, padding=1)\n",
    "    #     # self.conv4 = nn.Conv1d(in_channels=60, out_channels=60, kernel_size=3, padding=1)\n",
    "    #     # self.classifier = nn.Linear(2376, 7)\n",
    "    #     # self.out = nn.Conv1d(in_channels=60, out_channels=7, kernel_size=1, padding=0) \n",
    "\n",
    "    #     # self.fc1 = nn.Linear(in_features=60, out_features=1)\n",
    "        \n",
    "        \n",
    "    # def forward(self, x):\n",
    "    #     x = self.pool1(self.bn1(F.relu(self.conv1(x))))\n",
    "    #     x = self.pool2(self.bn2(F.relu(self.conv2(x))))\n",
    "    #     x = self.pool3(self.bn3(F.relu(self.conv3(x))))\n",
    "    #     x = self.pool4(self.bn4(F.relu(self.conv4(x))))\n",
    "    #     x = self.pool5(self.bn5(F.relu(self.conv5(x))))\n",
    "    #     # x = self.bn5(F.relu(self.conv5(x)))\n",
    "    #     # x = x.view(-1, 128*16) #Not sure what this is. what you doing ChatGPT?\n",
    "    #     x = self.fc1(x)\n",
    "    #     x = nn.functional.relu(x)\n",
    "    #     x = self.bn6(x)\n",
    "    #     x = self.fc2(x)\n",
    "    #     x = self.out(x)\n",
    "    #     x = self.softmax(x)\n",
    "    #     # x = F.relu(self.conv1(x))\n",
    "    #     # x = F.relu(self.conv2(x))\n",
    "    #     # x = F.relu(self.conv3(x))\n",
    "    #     # x = F.relu(self.conv4(x))\n",
    "    #     # x = self.classifier(x)\n",
    "    #     # # x = self.out(x)\n",
    "    #     # # x = self.fc1(x)\n",
    "    #     # x = x.view(-1, 7) \n",
    "    #     return x\n",
    "\n",
    "audio_model = AudioModel()\n",
    "audio_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"18\"></a>\n",
    "## Tuning for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T08:36:11.630018Z",
     "iopub.status.busy": "2022-07-01T08:36:11.628925Z",
     "iopub.status.idle": "2022-07-01T08:36:11.644878Z",
     "shell.execute_reply": "2022-07-01T08:36:11.643384Z",
     "shell.execute_reply.started": "2022-07-01T08:36:11.629973Z"
    }
   },
   "outputs": [],
   "source": [
    "# early_stop=EarlyStopping(monitor='val_acc',mode='auto',patience=5,restore_best_weights=True)\n",
    "# lr_reduction=ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.5,min_lr=0.00001)\n",
    "#this code block intially came before defining the model. \n",
    "\n",
    "\n",
    "# New Code from the denoising tutorial\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, patience: int = 3):\n",
    "        self.best_val = +np.inf\n",
    "        self.current_val = +np.inf\n",
    "        self.patience = patience\n",
    "        self.steps_since_last_best = 0\n",
    "        \n",
    "    def __call__(self, val) -> bool:\n",
    "        self.current_val = val\n",
    "        self._update_best()\n",
    "        if self.steps_since_last_best == self.patience:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _update_best(self):\n",
    "        if self.current_val < self.best_val:\n",
    "            tqdm.write(f\"New best: {self.current_val:.4f}\")\n",
    "            self.best_val=self.current_val\n",
    "            self.steps_since_last_best = 0\n",
    "        else:\n",
    "            self.steps_since_last_best+=1\n",
    "\n",
    "optim = torch.optim.Adam(params=audio_model.parameters())\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim,\n",
    "    \"min\",\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T09:22:45.681729Z",
     "iopub.status.busy": "2022-07-01T09:22:45.680903Z",
     "iopub.status.idle": "2022-07-01T10:39:10.74167Z",
     "shell.execute_reply": "2022-07-01T10:39:10.740358Z",
     "shell.execute_reply.started": "2022-07-01T09:22:45.681696Z"
    }
   },
   "outputs": [],
   "source": [
    "# history=model.fit(X_train, y_train, epochs=10, validation_data=(X_val,y_val), batch_size=64)\n",
    "# model.save(\"res_model.h5\")\n",
    "\n",
    "\n",
    "# New code to define a model trainer\n",
    "\n",
    "def train(model, optim, scheduler, criterion, train_loader) -> None:\n",
    "    device = next(iter(audio_model.parameters())).device # Check if this can be just model\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x,y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.reshape(8, 7, -1)\n",
    "        # print(f\"y_pred = {y_pred.shape}, y = {y.shape}\")\n",
    "        loss = criterion(y_pred.squeeze(), y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    scheduler.step(avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36857, 2376, 1), torch.Size([36857, 7]))"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 2376, 1), torch.Size([16, 7]))"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X1 = X_train[:16, :]\n",
    "# Y1 = y_train[:16, :]\n",
    "\n",
    "# X1.shape, Y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 2376])\n",
      "torch.Size([8, 7])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def get_data_loader(x,y,bs):\n",
    "    # Permute np.array from NLC TO NCL\n",
    "    x = torch.permute(torch.Tensor(x), dims=(0, 2, 1))\n",
    "    y = torch.Tensor(y)\n",
    "    return DataLoader(\n",
    "        TensorDataset(x, y),\n",
    "        batch_size=bs\n",
    "    )\n",
    "\n",
    "# train_loader = get_data_loader(X1, Y1, 8)\n",
    "train_loader = get_data_loader(X_train, y_train, 8)\n",
    "val_loader = get_data_loader(X_val, y_val, 8)\n",
    "test_loader = get_data_loader(X_test, y_test, 8)\n",
    "\n",
    "for x, y in train_loader:\n",
    "    break\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion) -> float: \n",
    "    model.eval()\n",
    "    device = next(iter(audio_model.parameters())).device\n",
    "    total_loss = 0\n",
    "    for x,y in val_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.reshape(8, 7, -1)\n",
    "        # print(f\"y_pred = {y_pred.shape}, y = {y.shape}\")\n",
    "        loss = criterion(y_pred.squeeze(), y)\n",
    "        # loss = criterion(y_pred.reshape(8, 7, -1), y) # Remove channel dim from y_pred\n",
    "        # loss = criterion(y_pred, y)\n",
    "        total_loss+=loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [03:06<12:27, 186.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best: 1.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [03:28<13:52, 208.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Garnet\\Documents\\UniversityOfCalgary\\Winter\\ENEL 645\\A2\\Assignment2\\speech-emotion-recognition-with-cnn.ipynb Cell 66\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train(audio_model, optim, lr_scheduler, criterion, train_loader)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     val_mse \u001b[39m=\u001b[39m validate(audio_model, val_loader, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m early_stopper(val_mse):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Garnet\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mc:\\Users\\Garnet\\Documents\\UniversityOfCalgary\\Winter\\ENEL 645\\A2\\Assignment2\\speech-emotion-recognition-with-cnn.ipynb Cell 66\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(model, val_loader, criterion)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_pred \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mreshape(\u001b[39m8\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# print(f\"y_pred = {y_pred.shape}, y = {y.shape}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Garnet\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Garnet\\Documents\\UniversityOfCalgary\\Winter\\ENEL 645\\A2\\Assignment2\\speech-emotion-recognition-with-cnn.ipynb Cell 66\u001b[0m in \u001b[0;36mAudioModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x))))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x))))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Garnet/Documents/UniversityOfCalgary/Winter/ENEL%20645/A2/Assignment2/speech-emotion-recognition-with-cnn.ipynb#Z1204sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool4(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn4(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(x))))\n",
      "File \u001b[1;32mc:\\Users\\Garnet\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Garnet\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Garnet\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\torch\\nn\\modules\\conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = \"./best_model.pt\"\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopper = EarlyStopper(patience=8)\n",
    "audio_model.to(device)\n",
    "\n",
    "for _ in tqdm(range(epochs)):\n",
    "    train(audio_model, optim, lr_scheduler, criterion, train_loader)\n",
    "    val_mse = validate(audio_model, val_loader, criterion)\n",
    "    if early_stopper(val_mse):\n",
    "        break\n",
    "    if early_stopper.steps_since_last_best == 0:\n",
    "        torch.save(audio_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = AudioModel()\n",
    "net = torch.load(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 8.0 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        # print(labels)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        l1 = labels\n",
    "        p1 = predicted\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"20\"></a>\n",
    "# Drawing Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"21\"></a>\n",
    "## Accuracy Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T12:20:10.574158Z",
     "iopub.status.busy": "2022-07-01T12:20:10.573231Z",
     "iopub.status.idle": "2022-07-01T12:20:10.66391Z",
     "shell.execute_reply": "2022-07-01T12:20:10.662422Z",
     "shell.execute_reply.started": "2022-07-01T12:20:10.574122Z"
    }
   },
   "outputs": [],
   "source": [
    "fig=px.line(history.history,y=['accuracy','val_accuracy'],\n",
    "           labels={'index':'epoch','value':'accuracy'},\n",
    "           title=f'According to the epoch accuracy and validation accuracy chart for the model')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"22\"></a>\n",
    "## Loss Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T12:20:17.070399Z",
     "iopub.status.busy": "2022-07-01T12:20:17.069939Z",
     "iopub.status.idle": "2022-07-01T12:20:17.156963Z",
     "shell.execute_reply": "2022-07-01T12:20:17.155215Z",
     "shell.execute_reply.started": "2022-07-01T12:20:17.070365Z"
    }
   },
   "outputs": [],
   "source": [
    "fig=px.line(history.history,y=['loss','val_loss'],\n",
    "           labels={'index':'epoch','value':'loss'},\n",
    "           title=f'According to the epoch loss and validation loss chart for the model')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"24\"></a>\n",
    "# Testing Model and Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T11:11:20.420397Z",
     "iopub.status.busy": "2022-07-01T11:11:20.420009Z",
     "iopub.status.idle": "2022-07-01T11:11:27.36655Z",
     "shell.execute_reply": "2022-07-01T11:11:27.365029Z",
     "shell.execute_reply.started": "2022-07-01T11:11:20.420368Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T11:22:36.286692Z",
     "iopub.status.busy": "2022-07-01T11:22:36.286312Z",
     "iopub.status.idle": "2022-07-01T11:22:36.296838Z",
     "shell.execute_reply": "2022-07-01T11:22:36.29535Z",
     "shell.execute_reply.started": "2022-07-01T11:22:36.286661Z"
    }
   },
   "outputs": [],
   "source": [
    "y_check=np.argmax(y_test,axis=1)\n",
    "y_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T11:12:26.770717Z",
     "iopub.status.busy": "2022-07-01T11:12:26.770294Z",
     "iopub.status.idle": "2022-07-01T11:12:33.968584Z",
     "shell.execute_reply": "2022-07-01T11:12:33.967149Z",
     "shell.execute_reply.started": "2022-07-01T11:12:26.770687Z"
    }
   },
   "outputs": [],
   "source": [
    "loss,accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"25\"></a>\n",
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T11:22:57.756325Z",
     "iopub.status.busy": "2022-07-01T11:22:57.755895Z",
     "iopub.status.idle": "2022-07-01T11:22:58.264051Z",
     "shell.execute_reply": "2022-07-01T11:22:58.262837Z",
     "shell.execute_reply.started": "2022-07-01T11:22:57.756296Z"
    }
   },
   "outputs": [],
   "source": [
    "conf=confusion_matrix(y_check,y_pred)\n",
    "cm=pd.DataFrame(\n",
    "    conf,index=[i for i in emotion_names],\n",
    "    columns=[i for i in emotion_names]\n",
    ")\n",
    "plt.figure(figsize=(12,7))\n",
    "ax=sns.heatmap(cm,annot=True,fmt='d')\n",
    "ax.set_title(f'confusion matrix for model ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T11:24:29.144726Z",
     "iopub.status.busy": "2022-07-01T11:24:29.144175Z",
     "iopub.status.idle": "2022-07-01T11:24:29.178353Z",
     "shell.execute_reply": "2022-07-01T11:24:29.17681Z",
     "shell.execute_reply.started": "2022-07-01T11:24:29.144659Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Model Confusion Matrix\\n',classification_report(y_check,y_pred,target_names=emotion_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"res_model.h5\")\n",
    "\n",
    "# save tflite model for android\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"res_model.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "fs = 44100  # Sample rate\n",
    "seconds = 3  # Duration of recording\n",
    "\n",
    "myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=1)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "path = 'test.wav'\n",
    "sf.write(path, myrecording, fs)  # Save as WAV file\n",
    "\n",
    "# play the sample\n",
    "sd.play(myrecording, fs)\n",
    "\n",
    "features=get_features(path)\n",
    "features=np.expand_dims(features,axis=0)\n",
    "features=np.expand_dims(features,axis=2)\n",
    "features.shape\n",
    "\n",
    "pred=model.predict(features)\n",
    "# print predicted emotion name and the probability\n",
    "print(emotion_names[np.argmax(pred)])\n",
    "print(np.max(pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dac71518e9b6653c862b69045eaaf0e53d81a7d8a744c209caca28ad60de33a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
